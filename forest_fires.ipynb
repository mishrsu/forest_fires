{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c1RByOWhsGtP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import math\n",
    "matplotlib.use(\"Agg\")\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3xqFHNkLiZ2a"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class Scaler():\n",
    "    # hint: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "    def _init_(self):\n",
    "        raise NotImplementedError\n",
    "    def _call_(self,features, is_train=False):\n",
    "        if is_train==True:\n",
    "            self.mean = np.mean(features, axis=0)\n",
    "            self.max= np.max(features,axis=0) \n",
    "            self.min= np.min(features,axis=0)\n",
    "            features = (features - self.min)/(self.max-self.min) \n",
    "        else:\n",
    "            features =(features - self.min)/(self.max-self.min)\n",
    "            \n",
    "        return features\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n7urZF4qijdq"
   },
   "outputs": [],
   "source": [
    "def get_features(csv_path,is_train=False,scaler=None):\n",
    "    '''\n",
    "    Description:\n",
    "    read input feature columns from csv file\n",
    "    manipulate feature columns, create basis functions, do feature scaling etc.\n",
    "    return a feature matrix (numpy array) of shape m x n \n",
    "    m is number of examples, n is number of features\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    csv_path: path to csv file\n",
    "    is_train: True if using training data (optional)\n",
    "    scaler: a class object for doing feature scaling (optional)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    help:\n",
    "    useful links: \n",
    "        * https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "        * https://www.geeksforgeeks.org/python-read-csv-using-pandas-read_csv/\n",
    "    '''\n",
    "    #path = str(csv_path)\n",
    "    \n",
    "    #df_data = pd.read_csv(filepath_or_buffer = \"%s\" % csv_path)\n",
    "  \n",
    "    df_data = pd.read_csv(filepath_or_buffer = csv_path)\n",
    "    \n",
    "    #pre- processing data\n",
    "    df_data = df_data.iloc[: , 1:]\n",
    "    df_data = df_data.dropna()\n",
    "    \n",
    "    #Feature Engineering\n",
    "    df_old = df_data\n",
    "    df_data[\"daynight\"] = df_data[\"daynight\"].apply(lambda val: 1 if val == \"D\" else 0)\n",
    "    df_data[\"satellite\"] = df_data[\"satellite\"].apply(lambda val: 1 if val == \"Terra\" else 0)\n",
    "    data = df_data.copy()\n",
    "    #Numerical Features for Date \n",
    "    df_data[\"acq_date\"] = pd.to_datetime(df_data[\"acq_date\"])\n",
    "    df_data[\"acq_day\"] = df_data[\"acq_date\"].apply(lambda row: row.day)\n",
    "    df_data[\"acq_date\"] = df_data[\"acq_date\"].apply(lambda row: row.month)\n",
    "    \n",
    "    #df_data[\"acq_year\"] = df_data[\"acq_date\"].apply(lambda row: row.year)\n",
    "    \n",
    "    df_data = df_data.drop([\"instrument\",\"version\"],axis=1)\n",
    "    df_data[\"confidence\"] = df_data[\"confidence\"].astype(\"float\")\n",
    "    df_data[\"latitude\"] = df_data[\"latitude\"]*(-1)\n",
    "    df_data[\"acq_time\"] = df_data[\"acq_time\"].astype(\"float\")\n",
    "    \n",
    "    #Drop co-related columns\n",
    "    #df_data[\"track\"] = df_data[\"track\"] - df_data[\"scan\"]\n",
    "    #df_data[\"track\"] = df_data[\"track\"]**2\n",
    "    #df_data[\"scan\"] = df_data[\"scan\"]**2\n",
    "    #df_data[\"confidence\"] = df_data[\"confidence\"]**2\n",
    "    #df_data[\"bright_t31\"] = df_data[\"bright_t31\"]**2\n",
    "   \n",
    "    #df_data[\"bright\"] = \n",
    "    #log normalization\n",
    "    #df_data[\"brightness\"]=np.log(df_data[\"brightness\"])\n",
    "    #df_data[\"bright_t31\"]=np.log(df_data[\"bright_t31\"])\n",
    "    #Normalise data\n",
    "    #df_data_norm = df_data/((df_data.max()-df_data.min())\n",
    "    #df_data_norm = df_data\n",
    "    #df_data[\"base\"] = 1\n",
    "    #scan and track are corelated\n",
    "    \n",
    "    #Normalize\n",
    "\n",
    "    #for i in range(len(df_data.columns)):\n",
    "    #    df_data.iloc[: , i] = (df_data.iloc[: , i] - df_data.iloc[: , i].mean())/df_data.iloc[: , i].std()\n",
    "\n",
    "    df_data[\"acq_day_period\"] = df_data[\"acq_day\"]\n",
    "    #df_data[\"acq_date_period\"] = df_data[\"acq_date\"]\n",
    "    df_data[\"track_period\"] = df_data[\"track\"]\n",
    "    #extract periodicity\n",
    "    for i in range(df_data.shape[0]):\n",
    "        df_data.loc[i, \"acq_day_period\"] = math.sin(df_data.loc[i, \"acq_day\"])\n",
    "        #df_data.loc[i, \"acq_date_period\"] = math.sin(df_data.loc[i, \"acq_date\"])\n",
    "        df_data.loc[i, \"track_period\"] = math.sin(df_data.loc[i, \"track\"])\n",
    "    #Normalize\n",
    "    df_data = (df_data-df_data.min())/(df_data.max()-df_data.min())\n",
    "    \n",
    "    df_data = df_data.drop([\"track\"],axis=1)\n",
    "        \n",
    "    df_data[\"daynight\"] = data [\"daynight\"]\n",
    "    df_data[\"satellite\"] = data[\"satellite\"]\n",
    "    df_data[\"base\"] = 1\n",
    "\n",
    "    first_column = df_data.pop('base')\n",
    "\n",
    "    df_data.insert(0, 'base', first_column)\n",
    "    basis_1 = True\n",
    "    basis_2 = False\n",
    "\n",
    "    if basis_1 == True:\n",
    "      df_data = df_data**2\n",
    "    \n",
    "    if basis_2 == True:\n",
    "      df_data = df_data**3\n",
    "    \n",
    "    if is_train == True:\n",
    "      df_data = df_data.drop([\"frp\"],axis=1)\n",
    "        \n",
    "    df_data = df_data.to_numpy()\n",
    "    return(df_data)\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Fx2UEmHyiobi"
   },
   "outputs": [],
   "source": [
    "feature_matrix = get_features(\"train.csv\",True)\n",
    "\n",
    "#df_features = df_features.Scaler()\n",
    "\n",
    "# feature_matrix.head()\n",
    "\n",
    "# feature_matrix.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5rgG1fzPi3uJ"
   },
   "outputs": [],
   "source": [
    "def get_targets(csv_path):\n",
    "    '''\n",
    "    Description:\n",
    "    read target outputs from the csv file\n",
    "    return a numpy array of shape m x 1\n",
    "    m is number of examples\n",
    "    '''\n",
    "    df_target = pd.read_csv(filepath_or_buffer = csv_path)\n",
    "    \n",
    "    #pre- processing data\n",
    "    df_target = df_target.dropna()\n",
    "    df_target = df_target[\"frp\"]\n",
    "\n",
    "    df_target = df_target.to_numpy()\n",
    "    return(df_target)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rivev5pvonmQ"
   },
   "outputs": [],
   "source": [
    "targets = get_targets(\"train.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9VcEyY0HisQV"
   },
   "outputs": [],
   "source": [
    "def analytical_solution(feature_matrix, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    implement analytical solution to obtain weights\n",
    "    as described in lecture 5d\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "    # Get transpose\n",
    "    feature_transpose = feature_matrix.T\n",
    "    \n",
    "    n = feature_matrix.shape[1]\n",
    "    # Multiply the matrices: Matrix1 and Matrix2\n",
    "    result = np.matmul(feature_transpose,feature_matrix);\n",
    "    \n",
    "    result = result + (np.identity(n))*C\n",
    "    #invert\n",
    "    result_inv = np.linalg.inv(result)\n",
    "    print(n)\n",
    "    output = np.matmul(result_inv,feature_transpose)\n",
    "    \n",
    "    weights = np.matmul(output,targets)\n",
    "    \n",
    "    \n",
    "    return weights\n",
    "\n",
    "    raise NotImplementedError "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyBGO_qFmYdd",
    "outputId": "3d89b851-6ef5-48a7-f2c9-cded88ed09b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -47.84793995,    5.14092064,   -5.4048751 , 1887.08509411,\n",
       "        244.03920169,   11.02879454,   14.16281237,   -1.92149527,\n",
       "          2.63845817, -127.94397905,  -18.81447254,   -7.7145926 ,\n",
       "         25.09225444,   91.78053648])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights = analytical_solution(feature_matrix, targets, 0)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9P-1oGV_qFzT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_predictions(feature_matrix, weights):\n",
    "    '''\n",
    "    description\n",
    "    return predictions given feature matrix and weights\n",
    "    return value: numpy array\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "    #test_features = get_features(feature_matrix)\n",
    "    \n",
    "    predictions = np.matmul(feature_matrix,weights)\n",
    "    return (predictions)\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgn1lN-iwO0v",
    "outputId": "d62eb286-0ad9-4e12-d151-0ae81b097bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dev_features = get_features(\"dev.csv\",True)\n",
    "dev_features.shape\n",
    "\n",
    "dev_targets = get_targets(\"dev.csv\")\n",
    "\n",
    "weights =  analytical_solution(dev_features, dev_targets, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwThyt7PwNKz",
    "outputId": "fd5656a7-9450-481a-8199-f21699faf8f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([159.51642724,  40.6739243 ,  48.7448923 , ..., -34.22917484,\n",
       "       113.49263834,  -9.37229991])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_predictions(dev_features, weights)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8Xh-RdS1ufE",
    "outputId": "1b84ad89-d641-4e36-eee8-dcae5aca7588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = get_features(\"train.csv\",True)\n",
    "targets = get_targets('train.csv')\n",
    "weights =   analytical_solution(feature_matrix, targets, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0we-VH6mwMdY"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Y4x4MSiPv63P"
   },
   "outputs": [],
   "source": [
    "def mse_loss(feature_matrix, weights, targets):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement mean squared error loss function\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    '''\n",
    "\n",
    "    predictions = np.matmul(feature_matrix,weights)\n",
    "    error = targets - predictions\n",
    "    squared_error = np.sum(np.square(error))\n",
    "    \n",
    "    mse = squared_error/len(targets)\n",
    "\n",
    "    return mse\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WMJHH-8xDiF",
    "outputId": "56defa61-83ba-4cd5-ed79-63e4b7a58f9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35088.52860675592"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mse_loss(dev_features, weights, dev_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnxFmOtNydet",
    "outputId": "e35fa8b7-9552-4035-a181-4ca4e7e258c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = get_features(\"dev.csv\",True)\n",
    "targets = get_targets('dev.csv')\n",
    "weights =   analytical_solution(feature_matrix, targets, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zhpScle12Uvb"
   },
   "outputs": [],
   "source": [
    "\n",
    "def l2_regularizer(weights):\n",
    "    '''\n",
    "    Description:\n",
    "    Implement l2 regularizer\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    weights: numpy array of shape n x 1\n",
    "    '''\n",
    "    \n",
    "    #df_reg = weights\n",
    "    #df_reg_sq = weights**2\n",
    "    l2_regular =np.sum(np.square(weights[:-1]))\n",
    "    #return(df_reg_sq.sum())\n",
    "    return l2_regular\n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ly5smNGx3AwK",
    "outputId": "0dd31ec9-04a8-40a6-9d14-66c18b30f9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4195613.494906143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(l2_regularizer(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gqOk-so-2W3L"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute the loss function: mse_loss + C * l2_regularizer\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: float (scalar)\n",
    "    '''\n",
    "    loss = mse_loss(feature_matrix, weights, targets)\n",
    "    reg = l2_regularizer(weights)\n",
    "    c_reg = C*reg\n",
    "    return(loss+c_reg)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dV52QeuT229U",
    "outputId": "76356f1a-7aa3-4bb0-b79b-d32f0363a581"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4230240.707823914"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(feature_matrix, weights, targets,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "z14IM35t2mcc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_random_batch(feature_matrix, targets, batch_size):\n",
    "    '''\n",
    "    Description\n",
    "    Batching -- Randomly sample batch_size number of elements from feature_matrix and targets\n",
    "    return a tuple: (sampled_feature_matrix, sampled_targets)\n",
    "    sampled_feature_matrix: numpy array of shape batch_size x n\n",
    "    sampled_targets: numpy array of shape batch_size x 1\n",
    "    '''\n",
    "\n",
    "    feature_random = np.random.choice(feature_matrix.shape[0], size=batch_size, replace=False)\n",
    "    \n",
    "    # sampled_feature_matrix = feature_matrix.loc[feature_random,:]\n",
    "    # sampled_targets=targets[feature_random]\n",
    "    '''\n",
    "    \n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    targets: numpy array of shape m x 1\n",
    "    batch_size: int\n",
    "    '''   \n",
    "\n",
    "\n",
    "    return (feature_matrix[feature_random],targets[feature_random])\n",
    "    # return (sampled_feature_matrix,sampled_targets)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4r9wt5tCbDH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJOM8K_x_9XH",
    "outputId": "5aeefda0-a452-484f-dc01-e512e560894c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 2.35572155e-02, 5.54438181e-01, ...,\n",
       "         5.87777778e-01, 2.24950017e-03, 3.28149994e-01],\n",
       "        [1.00000000e+00, 2.36360186e-02, 2.83248353e-01, ...,\n",
       "         2.50000000e-01, 1.27967311e-01, 9.02455996e-01],\n",
       "        [1.00000000e+00, 3.86042049e-02, 3.66626749e-01, ...,\n",
       "         5.87777778e-01, 2.24950017e-03, 3.28149994e-01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 6.13692321e-01, 8.38990108e-01, ...,\n",
       "         1.77777778e-02, 4.25594914e-04, 9.89623815e-02],\n",
       "        [1.00000000e+00, 5.03090221e-01, 8.76594292e-01, ...,\n",
       "         1.77777778e-02, 4.25594914e-04, 0.00000000e+00],\n",
       "        [1.00000000e+00, 6.17667279e-01, 7.38241519e-01, ...,\n",
       "         1.11111111e-03, 9.19974487e-01, 1.00000000e+00]]),\n",
       " array([2.50000e+01, 4.56000e+01, 5.78000e+01, 1.08000e+01, 9.44000e+01,\n",
       "        1.84000e+01, 8.47300e+02, 1.45000e+01, 3.13100e+02, 5.03000e+01,\n",
       "        3.99000e+01, 6.09000e+01, 8.50000e+00, 2.20000e+01, 1.05700e+02,\n",
       "        4.75000e+01, 2.09000e+01, 7.43000e+01, 6.57000e+01, 2.61700e+02,\n",
       "        9.40000e+00, 3.09000e+01, 1.83000e+01, 2.12400e+02, 5.86000e+01,\n",
       "        6.92000e+01, 6.40000e+00, 2.27000e+01, 2.25000e+01, 4.51000e+01,\n",
       "        1.93000e+01, 1.07600e+02, 2.40000e+01, 1.97000e+01, 2.21000e+01,\n",
       "        6.99000e+01, 2.38000e+01, 1.39000e+01, 2.38000e+01, 1.27900e+02,\n",
       "        3.09300e+02, 1.10916e+04, 5.87000e+01, 3.93800e+02, 1.57000e+01,\n",
       "        1.54800e+02, 1.77000e+01, 4.99000e+01, 4.23000e+01, 7.02000e+01,\n",
       "        5.21000e+01, 1.54300e+03, 3.34800e+02, 4.44000e+01, 6.22000e+01,\n",
       "        1.77000e+02, 3.86000e+01, 4.94000e+01, 2.05000e+01, 2.20000e+01,\n",
       "        5.16000e+01, 2.32000e+01, 3.25000e+01, 1.63000e+01, 7.58000e+01,\n",
       "        1.03000e+01, 4.06000e+01, 1.14000e+01, 2.11000e+01, 5.60000e+01,\n",
       "        3.08000e+01, 3.46000e+01, 6.24000e+01, 1.57000e+01, 1.23200e+02,\n",
       "        7.90000e+00, 2.10700e+02, 2.14700e+02, 2.80000e+01, 4.86000e+01,\n",
       "        5.50000e+00, 2.83000e+01, 8.60000e+00, 1.01000e+01, 1.36700e+02,\n",
       "        8.60000e+00, 1.03000e+01, 4.18000e+01, 2.25000e+01, 5.47000e+01,\n",
       "        0.00000e+00, 2.65000e+01, 1.80700e+02, 1.21000e+01, 2.22000e+01,\n",
       "        6.30000e+00, 2.15000e+01, 2.22000e+01, 9.15000e+01, 3.27000e+01]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_random_batch(feature_matrix, targets, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daiUwv11_9f9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CAcdJfP741C2"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n):\n",
    "    '''\n",
    "    Description:\n",
    "    initialize weights to some initial values\n",
    "    return value: numpy array of shape n x 1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments\n",
    "    n: int\n",
    "    '''\n",
    "    return(np.zeros((n, 1)))\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TDOgP2lg442O"
   },
   "outputs": [],
   "source": [
    "\n",
    "def early_stopping(arg_1=None, arg_2=None, arg_3=None, arg_n=None):\n",
    "    # allowed to modify argument list as per your need\n",
    "    # return True or False\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KAZWWNMt47Me"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "def plot_trainsize_losses():\n",
    "    '''\n",
    "    Description:\n",
    "    plot losses on the development set instances as a function of training set size \n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    # you are allowed to change the argument list any way you like \n",
    "    '''    \n",
    "\n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "v585koPs5rCS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def update_weights(weights, gradients, lr):\n",
    "    '''\n",
    "    Description:\n",
    "    update weights using gradient descent\n",
    "    retuen value: numpy matrix of shape nx1\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    # weights: numpy matrix of shape nx1\n",
    "    # gradients: numpy matrix of shape nx1\n",
    "    # lr: learning rate\n",
    "    '''    \n",
    "    \n",
    "    #Minimize the function\n",
    "    updated_weights = weights - gradients*lr\n",
    "    return(updated_weights)\n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdpEKNdO78Wt",
    "outputId": "63f14b7b-cde1-41a4-ba23-af660e4669f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Amsd9M3c5uue"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_gradients(feature_matrix, weights, targets, C=0.0):\n",
    "    '''\n",
    "    Description:\n",
    "    compute gradient of weights w.r.t. the loss_fn function implemented above\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "    feature_matrix: numpy array of shape m x n\n",
    "    weights: numpy array of shape n x 1\n",
    "    targets: numpy array of shape m x 1\n",
    "    C: weight for regularization penalty\n",
    "    return value: numpy array\n",
    "    '''\n",
    "    \n",
    "    predictions = np.matmul(feature_matrix,weights)\n",
    "    \n",
    "    error = (targets - predictions)\n",
    "    \n",
    "    gradient = weights*0\n",
    "    \n",
    "    feature_i = feature_matrix[:,0]\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "            \n",
    "        feature_i = feature_matrix[:,i]\n",
    "        gradient[i] = (np.matmul(feature_i.reshape(1,len(feature_i)),error))*(-2)\n",
    "        #Dont penalize intercept\n",
    "        if i == 0:\n",
    "            gradient[i] = gradient[i] \n",
    "        else:\n",
    "            gradient[i] = gradient[i] + (2*C*weights[i])\n",
    "    return (gradient/len(feature_i))\n",
    "    ##Implement ridge regression\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dS88ZntX5yvI",
    "outputId": "663829b8-6866-4454-9c0c-0fe4bcce52ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.72780215e-15, -1.42073029e-14,  6.09209148e-14,  5.60336026e-14,\n",
       "       -9.31999070e-15, -9.31999070e-15,  1.00161485e-13,  6.97862718e-14,\n",
       "        6.67174944e-14,  3.08014327e-14,  6.43306675e-14, -1.47755950e-15,\n",
       "        2.18224172e-14, -1.81853477e-14])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradients(feature_matrix, weights, targets, C=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MuakzB1w_NAP"
   },
   "outputs": [],
   "source": [
    "train_features = get_features(\"train.csv\",True)\n",
    "\n",
    "train_targets = get_targets(\"train.csv\")\n",
    "\n",
    "dev_features = get_features(\"dev.csv\",True)\n",
    "\n",
    "dev_targets = get_targets(\"dev.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gradient_descent(train_feature_matrix,  \n",
    "                        train_targets, \n",
    "                        dev_feature_matrix,\n",
    "                        dev_targets,\n",
    "                        lr=1,\n",
    "                        C=0.0,\n",
    "                        batch_size=100,\n",
    "                        max_steps=1000,\n",
    "                        eval_steps=50):\n",
    "    '''\n",
    "    feel free to significantly modify the body of this function as per your needs.\n",
    "    ** However **, you ought to make use of compute_gradients and update_weights function defined above\n",
    "    return your best possible estimate of LR weights\n",
    "\n",
    "    a sample code is as follows -- \n",
    "    '''\n",
    "\n",
    "        #get the closed form solution as starting point\n",
    "    c_r = C\n",
    "    df_weights = analytical_solution(train_features, train_targets, c_r)\n",
    "    print(df_weights)\n",
    "    weights = df_weights\n",
    "    gradient_i = weights\n",
    "    converged = False\n",
    "    step_size = lr\n",
    "    tolerance = 1200\n",
    "    features = train_feature_matrix \n",
    "    targets = train_targets\n",
    "    val_features = dev_feature_matrix\n",
    "    val_targets = dev_targets\n",
    "    while not converged:\n",
    "        counter=0\n",
    "        error = 0\n",
    "        error_eval = 0\n",
    "        toggle_old = True\n",
    "        error_old = 0\n",
    "        toggle = True\n",
    "        dev_loss = loss_fn(val_features, weights, val_targets,c_r)\n",
    "        train_loss = loss_fn(features, weights, targets,c_r)\n",
    "        print(\"dev loss: {} \\t train loss: {}\".format(dev_loss,train_loss))\n",
    "        error += (dev_loss + train_loss)/2\n",
    "        error_old = error\n",
    "        error_eval = error_old\n",
    "        for step in range(1,max_steps+1):\n",
    "\n",
    "            features,targets = sample_random_batch(train_features,train_targets,batch_size)\n",
    "            val_features,val_targets = sample_random_batch(dev_feature_matrix,dev_targets,batch_size)\n",
    "            grad = compute_gradients(features, weights, targets)  \n",
    "            weights = update_weights(weights, grad, step_size)\n",
    "            \n",
    "\n",
    "            if step%eval_steps == 0:\n",
    "                dev_loss = loss_fn(val_features, weights, val_targets,c_r)\n",
    "                train_loss = loss_fn(features, weights, targets,c_r)\n",
    "                print(\"dev loss: {} \\t train loss: {}\".format(dev_loss,train_loss))\n",
    "                error_eval = (dev_loss + train_loss)/2\n",
    "                if error_eval < tolerance:\n",
    "                    converged = True\n",
    "                    break\n",
    "                error += error_eval\n",
    "                counter = counter+1\n",
    "\n",
    "                \n",
    "\n",
    "                #5 fold cross validation\n",
    "                if counter == 5:\n",
    "                    if toggle == True:\n",
    "                        c_r = c_r + 0.000001\n",
    "                    if toggle == False:\n",
    "                        c_r = c_r - 0.000001\n",
    "                    error = error/5\n",
    "                    error_eval = error\n",
    "                    error = 0\n",
    "                    toggle_old = toggle\n",
    "                    if error < error_old:\n",
    "                        toggle = toggle_old\n",
    "                    \n",
    "                    if error > error_old:\n",
    "                        toggle = !toggle\n",
    "                        \n",
    "                    toggle_old = toggle\n",
    "                    error_old = error\n",
    "\n",
    "\n",
    "                    #print(\"dev loss: {} \\t train loss: {}\".format(dev_loss,train_loss))\n",
    "\n",
    "\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[ -47.84794166    5.14092199   -5.40487476 1887.08507373  244.03920138\n",
      "   11.02879444   14.16281222   -1.92149529    2.63845957 -127.94396024\n",
      "  -18.81447284   -7.71459248   25.09225439   91.78053666]\n",
      "dev loss: 35092.16935909341 \t train loss: 22502.20268550447\n",
      "dev loss: 3617.618129470813 \t train loss: 1501.2233181599888\n",
      "dev loss: 4110.245859000859 \t train loss: 3018.6955398123455\n",
      "dev loss: 85931.78825639315 \t train loss: 22663.298990054343\n",
      "dev loss: 1293.7625749640097 \t train loss: 27132.227183672163\n",
      "dev loss: 1957.9345789548106 \t train loss: 1566.6904127018083\n",
      "dev loss: 3903.470226041941 \t train loss: 1820.0470630980867\n",
      "dev loss: 905.3199919060938 \t train loss: 1181.8355355221665\n"
     ]
    }
   ],
   "source": [
    "    gradient_descent_soln = do_gradient_descent(train_features, \n",
    "                        train_targets, \n",
    "                        dev_features,\n",
    "                        dev_targets,\n",
    "                        1e-3,\n",
    "                        0.000001,\n",
    "                        32,\n",
    "                        20000,\n",
    "                        5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -48.06083736,    4.9842618 ,   -5.47899193, 1886.85184935,\n",
       "        243.99344925,   10.93668191,   14.12085947,   -1.99468213,\n",
       "          2.45252461, -128.11269977,  -18.81982162,   -7.80033613,\n",
       "         25.05583894,   91.60064416])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 99.90678304, 440.80637908, -14.35620193, ...,  -3.33183013,\n",
       "       -19.79507546,  82.93103167])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = get_features(\"test.csv\",False)\n",
    "frp = (get_predictions(test_features, gradient_descent_soln))\n",
    "frp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 99.90678304 440.80637908 -14.35620193 ...  -3.33183013 -19.79507546\n",
      "  82.93103167]\n"
     ]
    }
   ],
   "source": [
    "# display the array\n",
    "print(frp)\n",
    "  \n",
    "# convert array into dataframe\n",
    "DF = pd.DataFrame(frp)\n",
    "  \n",
    "# save the dataframe as a csv file\n",
    "DF.to_csv(\"frp_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
